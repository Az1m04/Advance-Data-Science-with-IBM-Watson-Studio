{"cells": [{"metadata": {}, "cell_type": "code", "source": "!pip install pyspark==2.4.5", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Collecting pyspark==2.4.5\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 217.8MB 9.3MB/s eta 0:00:013   |\u2588\u258c                              | 10.4MB 6.5MB/s eta 0:00:32     |\u2588\u2588\u2588\u2588\u258e                           | 28.9MB 28.8MB/s eta 0:00:07     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                      | 67.6MB 33.9MB/s eta 0:00:055MB 34.8MB/s eta 0:00:03\ufffd\ufffd\u2588\u2589              | 121.1MB 34.8MB/s eta 0:00:03     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     | 184.2MB 9.2MB/s eta 0:00:04\n\u001b[?25hCollecting py4j==0.10.7 (from pyspark==2.4.5)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 32.7MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.7 pyspark-2.4.5\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()", "execution_count": 2, "outputs": [{"output_type": "error", "ename": "NameError", "evalue": "name 'SparkContext' is not defined", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "\u001b[0;32m<ipython-input-2-9d25752ae248>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mNameError\u001b[0m: name 'SparkContext' is not defined"]}]}, {"metadata": {}, "cell_type": "code", "source": "try:\n    from pyspark import SparkContext, SparkConf\n    from pyspark.sql import SparkSession\nexcept ImportError as e:\n    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "lx=[1,2,3,4,5,6,7,8,9,10]\nly=[7,6,5,4,5,6,7,8,9,10]\nrdx=sc.parallelize(lx)\nrdy=sc.parallelize(ly)", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "meanx=rdx.sum()/float(rdx.count())\nmeany=rdy.sum()/float(rdy.count())\n\nprint(meanx)\nprint(meany)", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "5.5\n6.7\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "rdxy=rdx.zip(rdy)\ncovXY = rdxy.map(lambda x_y : (x_y[0]-meanx)*(x_y[1]-meany)).sum()/rdxy.count()\ncovXY", "execution_count": 11, "outputs": [{"output_type": "execute_result", "execution_count": 11, "data": {"text/plain": "3.65"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "from math import sqrt\nn = rdxy.count()\nsdX = sqrt(rdx.map(lambda x : pow(x-meanx,2)).sum()/n)\nsdY = sqrt(rdy.map(lambda x : pow(x-meany,2)).sum()/n)\nprint (sdX)\nprint (sdY)", "execution_count": 13, "outputs": [{"output_type": "stream", "text": "2.8722813232690143\n1.7916472867168918\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "corrXY = covXY / (sdX * sdY)\ncorrXY", "execution_count": 14, "outputs": [{"output_type": "execute_result", "execution_count": 14, "data": {"text/plain": "0.7092729120837249"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}